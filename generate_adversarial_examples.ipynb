{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import foolbox\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# from model import CNN\n",
    "from model_mnist import CNN\n",
    "from classifier_model import Model_A,Model_B,Model_C,Model_D,Model_E,Model_F\n",
    "\n",
    "from torchsummary import summary\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes in the dataset\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, num_workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('one', 'two', 'three', 'four', 'five', \n",
    "           'six', 'seven', 'eight', 'nine', 'ten')\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, num_workers = 0, pin_memory =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, num_workers = 0, pin_memory =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = os.path.abspath('./adversarial_mnist')\n",
    "if not os.path.exists(model_folder):\n",
    "    os.mkdir(model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data f-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=True, download=True, transform=transform_test)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('t-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot')\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, num_workers = 4, pin_memory =True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, num_workers = 4, pin_memory =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = os.path.abspath('./adversarial_F_mnist')\n",
    "if not os.path.exists(model_folder):\n",
    "    os.mkdir(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load classification model cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN()\n",
    "\n",
    "model.load_state_dict(torch.load('./checkpoints/mnist.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load classification model mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model_A()\n",
    "\n",
    "model.load_state_dict(torch.load('./checkpoints/mnist_model_A.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load classification model F-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model_B()\n",
    "\n",
    "model.load_state_dict(torch.load('./checkpoints/F_mnist_model_B.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send the model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model_B(\n",
       "  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(8, 8), stride=(2, 2), padding=(3, 3))\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(6, 6), stride=(2, 2))\n",
       "  (relu2): ReLU()\n",
       "  (conv3): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (relu3): ReLU()\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_steps = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load adversarial generation tool - torchattacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm_adv = []\n",
    "fgsm_index = []\n",
    "fgsm_label = []\n",
    "# attack = torchattacks.FGSM(model, eps=0.3)\n",
    "attack = torchattacks.RFGSM(model, eps=0.3, alpha= 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "is_adv = []\n",
    "# print(len(test_dataset))\n",
    "for idx, data in enumerate(test_loader):\n",
    "    img, label = data\n",
    "    img, label = img.to(device),label.to(device)\n",
    "#     print(type(label))\n",
    "    adv_image = attack(img, label)\n",
    "    \n",
    "#     if idx % display_steps == 0:\n",
    "#             save_image(img.data, './adv_images/fmnist/rfgsm_orig_{}.png'.format(idx), nrow=8, normalize=True)\n",
    "#             save_image(adv_image.data, './adv_images/fmnist/rfgsm_adv_{}.png'.format(idx), nrow=8, normalize=True)\n",
    "            \n",
    "    for i in adv_image:\n",
    "        fgsm_adv.append(i)\n",
    "    for i in label:\n",
    "        fgsm_label.append(i)\n",
    "        \n",
    "for i in range(len(test_dataset)):\n",
    "    fgsm_index.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of adversarial images : 10000\n"
     ]
    }
   ],
   "source": [
    "print('length of adversarial images : {}'.format(len(fgsm_adv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./adversarial_F_mnist/RFGSM_WHITE_D_0.3_indexs.pickle', 'wb') as fp:\n",
    "    pickle.dump(fgsm_index, fp)\n",
    "\n",
    "with open ('./adversarial_F_mnist/RFGSM_WHITE_D_0.3_adv_images.pickle', 'wb') as fp:\n",
    "    pickle.dump(fgsm_adv, fp)\n",
    "    \n",
    "with open ('./adversarial_F_mnist/RFGSM_WHITE_D_0.3_adv_label.pickle', 'wb') as fp:\n",
    "    pickle.dump(fgsm_label, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load adversarial generation tool - Foolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel = foolbox.models.PyTorchModel(model, bounds = (0, 1), device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm_adv = []\n",
    "fgsm_index = []\n",
    "fgsm_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9788, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "labels = []\n",
    "is_adv = []\n",
    "adv_sum = 0\n",
    "# print(len(test_dataset))\n",
    "for idx, data in enumerate(test_loader):\n",
    "    img, label = data\n",
    "    img, label = img.to(device),label.to(device)\n",
    "    # print(label.shape)\n",
    "    # print(img.shape)\n",
    "    # print(label)\n",
    "    #image = img.detach().numpy()\n",
    "    #apply attack on source image\n",
    "    attack = foolbox.attacks.FGSM()\n",
    "    raw_advs, clipped_advs, success = attack(fmodel, img, label, epsilons=0.3)\n",
    "    adv_sum += torch.sum(success)\n",
    "    #adversarial = attack(fmodel, image, label, epsilons=0.2)\n",
    "#     for i, s in enumerate(success):\n",
    "#         if s == False:\n",
    "#             continue\n",
    "#         else:\n",
    "#             fgsm_adv.append(clipped_advs[i])\n",
    "#             fgsm_index.append(idx*batch_size+i)\n",
    "#             fgsm_label.append(label[i].item())\n",
    "#             print('image {} save'.format(idx*batch_size+i))\n",
    "    if idx % display_steps == 0:\n",
    "        save_image(img.data, './adv_images/mnist/foolbox_orig_{}.png'.format(idx), nrow=8, normalize=True)\n",
    "        save_image(clipped_advs.data, './adv_images/mnist/foolbox_adv_{}.png'.format(idx), nrow=8, normalize=True)\n",
    "    \n",
    "#     for i in raw_advs:\n",
    "#         fgsm_adv.append(i)\n",
    "#     for i in label:\n",
    "#         fgsm_label.append(i)\n",
    "        \n",
    "# for i in range(len(test_dataset)):\n",
    "#     fgsm_index.append(i)\n",
    "    \n",
    "    \n",
    "print(adv_sum*1.0/len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of adversarial images : 1483\n"
     ]
    }
   ],
   "source": [
    "print('length of adversarial images : {}'.format(len(fgsm_adv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "while(i<len(fgsm_adv)/batch_size):\n",
    "    save_image(fgsm_adv[i*batch_size:i*batch_size+batch_size], './adv_images/fmnist/foolbox_success_adv_{}.png'.format(i), nrow=8, normalize=True)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cifar10 save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./adversarial/FGSM_0.3_indexs.pickle', 'wb') as fp:\n",
    "    pickle.dump(fgsm_index, fp)\n",
    "\n",
    "with open ('./adversarial/FGSM_0.3_adv_images.pickle', 'wb') as fp:\n",
    "    pickle.dump(fgsm_adv, fp)\n",
    "    \n",
    "with open ('./adversarial/FGSM_0.3_adv_label.pickle', 'wb') as fp:\n",
    "    pickle.dump(fgsm_label, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mnist save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./adversarial_mnist/FGSM_0.3_indexs.pickle', 'wb') as fp:\n",
    "    pickle.dump(fgsm_index, fp)\n",
    "\n",
    "with open ('./adversarial_mnist/FGSM_0.3_adv_images.pickle', 'wb') as fp:\n",
    "    pickle.dump(fgsm_adv, fp)\n",
    "    \n",
    "with open ('./adversarial_mnist/FGSM_0.3_adv_label.pickle', 'wb') as fp:\n",
    "    pickle.dump(fgsm_label, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f-mnist save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./adversarial_F_mnist/FGSM_WHITE_A_0.3_success_adv_indexs.pickle', 'wb') as fp:\n",
    "    pickle.dump(fgsm_index, fp)\n",
    "\n",
    "with open ('./adversarial_F_mnist/FGSM_WHITE_A_0.3_success_adv_images.pickle', 'wb') as fp:\n",
    "    pickle.dump(fgsm_adv, fp)\n",
    "    \n",
    "with open ('./adversarial_F_mnist/FGSM_WHITE_A_0.3_success_adv_label.pickle', 'wb') as fp:\n",
    "    pickle.dump(fgsm_label, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "tensor(7961, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "labels = []\n",
    "is_adv = []\n",
    "adv_sum = 0\n",
    "print(len(train_dataset))\n",
    "for idx, data in enumerate(train_loader):\n",
    "    img, label = data\n",
    "    img, label = img.to(device),label.to(device)\n",
    "    # print(label.shape)\n",
    "    # print(img.shape)\n",
    "    # print(label)\n",
    "    #image = img.detach().numpy()\n",
    "    #apply attack on source image\n",
    "    attack = foolbox.attacks.FGSM()\n",
    "    raw_advs, clipped_advs, success = attack(fmodel, img, label, epsilons=0.3)\n",
    "    #adversarial = attack(fmodel, image, label, epsilons=0.2)\n",
    "    for i,raw in enumerate(raw_advs):\n",
    "        fgsm_adv.append(raw)\n",
    "        fgsm_label.append(label[i].item())\n",
    "        \n",
    "    adv_sum += torch.sum(success)\n",
    "for i in range(len(train_dataset)):\n",
    "    fgsm_index.append(i)\n",
    "print(adv_sum)\n",
    "#     for i, s in enumerate(success):\n",
    "#         if s == False:\n",
    "# #             print('image {} false'.format(idx*batch_size+i))\n",
    "#             continue\n",
    "#         else:\n",
    "#             fgsm_adv.append(clipped_advs[i])\n",
    "# #             print(clipped_advs[i].shape)\n",
    "#             fgsm_index.append(idx*batch_size+i)\n",
    "#             fgsm_label.append(label[i].item())\n",
    "# #             print('image {} save'.format(idx*batch_size+i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of adversarial images : 60000\n"
     ]
    }
   ],
   "source": [
    "print('length of adversarial images : {}'.format(len(fgsm_adv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./adversarial_mnist/FGSM_0.3_train_indexs.pickle', 'wb') as fp:\n",
    "    pickle.dump(fgsm_index, fp)\n",
    "\n",
    "with open ('./adversarial_mnist/FGSM_0.3_train_adv_images.pickle', 'wb') as fp:\n",
    "    pickle.dump(fgsm_adv, fp)\n",
    "    \n",
    "with open ('./adversarial_mnist/FGSM_0.3_train_adv_label.pickle', 'wb') as fp:\n",
    "    pickle.dump(fgsm_label, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepFool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_adv = []\n",
    "DF_index = []\n",
    "DF_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in enumerate(test_loader):\n",
    "    img, label = data\n",
    "    img, label = img.to(device),label.to(device)\n",
    "    # print(label.shape)\n",
    "    # print(img.shape)\n",
    "    # print(label)\n",
    "    #image = img.detach().numpy()\n",
    "    #apply attack on source image\n",
    "    attack = foolbox.attacks.L2DeepFoolAttack()\n",
    "    raw_advs, clipped_advs, success = attack(fmodel, img, label, epsilons=0.3)\n",
    "    #adversarial = attack(fmodel, image, label, epsilons=0.2)\n",
    "#     for i, s in enumerate(success):\n",
    "#         if s == False:\n",
    "#             print('image {} false'.format(idx*batch_size+i))\n",
    "#             continue\n",
    "#         else:\n",
    "#             DF_adv.append(clipped_advs[i])\n",
    "# #             print(clipped_advs[i].shape)\n",
    "#             DF_index.append(idx*batch_size+i)\n",
    "#             DF_label.append(label[i].item())\n",
    "#             print('image {} save'.format(idx*batch_size+i))\n",
    "    for i in raw_advs:\n",
    "        DF_adv.append(i)\n",
    "    for i in label:\n",
    "        DF_label.append(i)\n",
    "        \n",
    "for i in range(len(test_dataset)):\n",
    "    DF_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of adversarial images : 10000\n"
     ]
    }
   ],
   "source": [
    "print('length of adversarial images : {}'.format(len(DF_adv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./adversarial_mnist/DF_0.3_indexs.pickle', 'wb') as fp:\n",
    "    pickle.dump(DF_index, fp)\n",
    "\n",
    "with open ('./adversarial_mnist/DF_0.3_adv_images.pickle', 'wb') as fp:\n",
    "    pickle.dump(DF_adv, fp)\n",
    "    \n",
    "with open ('./adversarial_mnist/DF_0.3_adv_label.pickle', 'wb') as fp:\n",
    "    pickle.dump(DF_label, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./adversarial/DF_0.2_indexs.pickle', 'wb') as fp:\n",
    "    pickle.dump(DF_index, fp)\n",
    "\n",
    "with open ('./adversarial/DF_0.2_adv_images.pickle', 'wb') as fp:\n",
    "    pickle.dump(DF_adv, fp)\n",
    "    \n",
    "with open ('./adversarial/DF_0.2_adv_label.pickle', 'wb') as fp:\n",
    "    pickle.dump(DF_label, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model_D()\n",
    "\n",
    "model.load_state_dict(torch.load('./checkpoints/F_mnist_model_D.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model_D(\n",
       "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=200, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "CW_adv = []\n",
    "CW_clip_adv = []\n",
    "CW_success = []\n",
    "CW_index = []\n",
    "CW_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel = foolbox.models.PyTorchModel(model, bounds = (0, 1), device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "# print(len(test_dataset))\n",
    "for idx, data in enumerate(test_loader):\n",
    "    img, label = data\n",
    "    img, label = img.to(device),label.to(device)\n",
    "    # print(label.shape)\n",
    "    # print(img.shape)\n",
    "    # print(label)\n",
    "    #image = img.detach().numpy()\n",
    "    #apply attack on source image\n",
    "    attack = foolbox.attacks.L2CarliniWagnerAttack(binary_search_steps=1,steps=100,initial_const=100,stepsize = 10)\n",
    "    raw_advs, clipped_advs, success = attack(fmodel, img, label, epsilons=0.3)\n",
    "    #adversarial = attack(fmodel, image, label, epsilons=0.2)\n",
    "#     for i, s in enumerate(success):\n",
    "#         if s == False:\n",
    "#             print('image {} false'.format(idx*64+i))\n",
    "#             continue\n",
    "#         else:\n",
    "#             CW_adv.append(clipped_advs[i])\n",
    "# #             print(clipped_advs[i].shape)\n",
    "#             CW_index.append(idx*64+i)\n",
    "#             CW_label.append(label[i].item())\n",
    "#             print('image {} save'.format(idx*64+i))\n",
    "#     if idx % display_steps == 0:\n",
    "#             save_image(img.data, './adv_images/fmnist/CW_orig_{}.png'.format(idx), nrow=8, normalize=True)\n",
    "#             save_image(clipped_advs.data, './adv_images/fmnist/CW_adv_{}.png'.format(idx), nrow=8, normalize=True)\n",
    "            \n",
    "    for i in raw_advs:\n",
    "        CW_adv.append(i)\n",
    "    for i in clipped_advs:\n",
    "        CW_clip_adv.append(i)\n",
    "    for i in success:\n",
    "        CW_success.append(i)\n",
    "    for i in label:\n",
    "        CW_label.append(i)\n",
    "#     print(\"image {} saved\".format(idx))\n",
    "        \n",
    "for i in range(len(test_dataset)):\n",
    "    CW_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1918\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in CW_success:\n",
    "    if i == True:\n",
    "        sum+=1\n",
    "print(sum/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of adversarial images : 10000\n"
     ]
    }
   ],
   "source": [
    "print('length of adversarial images : {}'.format(len(CW_clip_adv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./adversarial_mnist/CW_A_0.3_indexs.pickle', 'wb') as fp:\n",
    "    pickle.dump(CW_index, fp)\n",
    "    \n",
    "# with open ('./adversarial_mnist/CW_0.3_success.pickle', 'wb') as fp:\n",
    "#     pickle.dump(CW_success, fp)\n",
    "    \n",
    "# with open ('./adversarial_mnist/CW_0.3_adv_images.pickle', 'wb') as fp:\n",
    "#     pickle.dump(CW_adv, fp)\n",
    "    \n",
    "with open ('./adversarial_mnist/CW_A_0.3_adv_images.pickle', 'wb') as fp:\n",
    "    pickle.dump(CW_clip_adv, fp)\n",
    "    \n",
    "with open ('./adversarial_mnist/CW_A_0.3_adv_label.pickle', 'wb') as fp:\n",
    "    pickle.dump(CW_label, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLEVERHANS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleverhans.torch.attacks.carlini_wagner_l2 import carlini_wagner_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model_A()\n",
    "\n",
    "model.load_state_dict(torch.load('./checkpoints/F_mnist_model_A.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model_A(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (relu2): ReLU()\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "CW_adv = []\n",
    "CW_index = []\n",
    "CW_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 0 saved\n",
      "image 1 saved\n",
      "image 2 saved\n",
      "image 3 saved\n",
      "image 4 saved\n",
      "image 5 saved\n",
      "image 6 saved\n",
      "image 7 saved\n",
      "image 8 saved\n",
      "image 9 saved\n",
      "image 10 saved\n",
      "image 11 saved\n",
      "image 12 saved\n",
      "image 13 saved\n",
      "image 14 saved\n",
      "image 15 saved\n",
      "image 16 saved\n",
      "image 17 saved\n",
      "image 18 saved\n",
      "image 19 saved\n",
      "image 20 saved\n",
      "image 21 saved\n",
      "image 22 saved\n",
      "image 23 saved\n",
      "image 24 saved\n",
      "image 25 saved\n",
      "image 26 saved\n",
      "image 27 saved\n",
      "image 28 saved\n",
      "image 29 saved\n",
      "image 30 saved\n",
      "image 31 saved\n",
      "image 32 saved\n",
      "image 33 saved\n",
      "image 34 saved\n",
      "image 35 saved\n",
      "image 36 saved\n",
      "image 37 saved\n",
      "image 38 saved\n",
      "image 39 saved\n",
      "image 40 saved\n",
      "image 41 saved\n",
      "image 42 saved\n",
      "image 43 saved\n",
      "image 44 saved\n",
      "image 45 saved\n",
      "image 46 saved\n",
      "image 47 saved\n",
      "image 48 saved\n",
      "image 49 saved\n",
      "image 50 saved\n",
      "image 51 saved\n",
      "image 52 saved\n",
      "image 53 saved\n",
      "image 54 saved\n",
      "image 55 saved\n",
      "image 56 saved\n",
      "image 57 saved\n",
      "image 58 saved\n",
      "image 59 saved\n",
      "image 60 saved\n",
      "image 61 saved\n",
      "image 62 saved\n",
      "image 63 saved\n",
      "image 64 saved\n",
      "image 65 saved\n",
      "image 66 saved\n",
      "image 67 saved\n",
      "image 68 saved\n",
      "image 69 saved\n",
      "image 70 saved\n",
      "image 71 saved\n",
      "image 72 saved\n",
      "image 73 saved\n",
      "image 74 saved\n",
      "image 75 saved\n",
      "image 76 saved\n",
      "image 77 saved\n",
      "image 78 saved\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "labels = []\n",
    "# print(len(test_dataset))\n",
    "for idx, data in enumerate(test_loader):\n",
    "    img, label = data\n",
    "    img, label = img.to(device),label.to(device)\n",
    "    \n",
    "    adv_img = carlini_wagner_l2(model_fn=model,x=img,n_classes=10,lr=10.0,max_iterations=100,binary_search_steps=1,initial_const=100)\n",
    "            \n",
    "    for i in adv_img:\n",
    "        CW_adv.append(i)\n",
    "    for i in label:\n",
    "        CW_label.append(i)\n",
    "    print(\"image {} saved\".format(idx))\n",
    "        \n",
    "for i in range(len(test_dataset)):\n",
    "    CW_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./adversarial_F_mnist/CH_CW_A_0.3_indexs.pickle', 'wb') as fp:\n",
    "    pickle.dump(CW_index, fp)\n",
    "    \n",
    "# with open ('./adversarial_mnist/CW_0.3_success.pickle', 'wb') as fp:\n",
    "#     pickle.dump(CW_success, fp)\n",
    "    \n",
    "# with open ('./adversarial_mnist/CW_0.3_adv_images.pickle', 'wb') as fp:\n",
    "#     pickle.dump(CW_adv, fp)\n",
    "    \n",
    "with open ('./adversarial_F_mnist/CH_CW_A_0.3_adv_images.pickle', 'wb') as fp:\n",
    "    pickle.dump(CW_adv, fp)\n",
    "    \n",
    "with open ('./adversarial_F_mnist/CH_CW_A_0.3_adv_label.pickle', 'wb') as fp:\n",
    "    pickle.dump(CW_label, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581\n"
     ]
    }
   ],
   "source": [
    "real_fake = []\n",
    "real_label = []\n",
    "f1 = open ('./adversarial_mnist/CW_0.3_adv_images.pickle', 'rb')\n",
    "info = pickle.load(f1)\n",
    "f2 = open ('./adversarial_mnist/CW_0.3_success.pickle', 'rb')\n",
    "suc = pickle.load(f2)\n",
    "f3 = open ('./adversarial_mnist/CW_0.3_adv_label.pickle', 'rb')\n",
    "label = pickle.load(f3)\n",
    "for i,j,k in zip(suc,info,label):\n",
    "    if i == True:\n",
    "        real_fake.append(j)\n",
    "        real_label.append(k)\n",
    "f1.close()\n",
    "f2.close()\n",
    "f3.close()\n",
    "\n",
    "print(len(real_fake))\n",
    "\n",
    "with open ('./adversarial_mnist/CW_0.3_real_adv_images.pickle', 'wb') as fp:\n",
    "    pickle.dump(real_fake, fp)\n",
    "with open ('./adversarial_mnist/CW_0.3_real_adv_label.pickle', 'wb') as fp:\n",
    "    pickle.dump(real_label, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./adversarial/CW_0.2_indexs.pickle', 'wb') as fp:\n",
    "    pickle.dump(CW_index, fp)\n",
    "\n",
    "with open ('./adversarial/CW_0.2_adv_images.pickle', 'wb') as fp:\n",
    "    pickle.dump(CW_adv, fp)\n",
    "    \n",
    "with open ('./adversarial/CW_0.2_adv_label.pickle', 'wb') as fp:\n",
    "    pickle.dump(CW_label, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6191, 0.6235, 0.6470,  ..., 0.5374, 0.4947, 0.4546],\n",
      "         [0.5962, 0.5917, 0.6251,  ..., 0.5325, 0.4890, 0.4676],\n",
      "         [0.5922, 0.5925, 0.6186,  ..., 0.5467, 0.5090, 0.4704],\n",
      "         ...,\n",
      "         [0.2663, 0.1649, 0.1215,  ..., 0.1489, 0.0511, 0.1569],\n",
      "         [0.2391, 0.1921, 0.1374,  ..., 0.1021, 0.1136, 0.0786],\n",
      "         [0.2118, 0.2195, 0.1762,  ..., 0.0942, 0.1332, 0.0825]],\n",
      "\n",
      "        [[0.4387, 0.4352, 0.4543,  ..., 0.3724, 0.3571, 0.3332],\n",
      "         [0.4397, 0.4312, 0.4470,  ..., 0.3718, 0.3559, 0.3450],\n",
      "         [0.4317, 0.4285, 0.4350,  ..., 0.3851, 0.3717, 0.3476],\n",
      "         ...,\n",
      "         [0.4859, 0.3925, 0.3446,  ..., 0.3801, 0.2512, 0.3335],\n",
      "         [0.4544, 0.3996, 0.3335,  ..., 0.3218, 0.3215, 0.2512],\n",
      "         [0.4192, 0.4113, 0.3491,  ..., 0.3024, 0.3293, 0.2630]],\n",
      "\n",
      "        [[0.1922, 0.1840, 0.2000,  ..., 0.1411, 0.1420, 0.1294],\n",
      "         [0.2004, 0.1565, 0.1765,  ..., 0.1228, 0.1260, 0.1346],\n",
      "         [0.1843, 0.1308, 0.1414,  ..., 0.1341, 0.1335, 0.1290],\n",
      "         ...,\n",
      "         [0.6944, 0.5806, 0.5371,  ..., 0.5721, 0.4231, 0.4981],\n",
      "         [0.6592, 0.5801, 0.5178,  ..., 0.5099, 0.4940, 0.4200],\n",
      "         [0.6279, 0.5848, 0.5181,  ..., 0.4860, 0.5058, 0.4308]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[0.6157, 0.6235, 0.6431,  ..., 0.5373, 0.4941, 0.4510],\n",
      "         [0.5961, 0.5882, 0.6235,  ..., 0.5294, 0.4863, 0.4667],\n",
      "         [0.5882, 0.5922, 0.6157,  ..., 0.5451, 0.5059, 0.4667],\n",
      "         ...,\n",
      "         [0.2627, 0.1647, 0.1176,  ..., 0.1451, 0.0510, 0.1569],\n",
      "         [0.2353, 0.1882, 0.1373,  ..., 0.1020, 0.1098, 0.0784],\n",
      "         [0.2078, 0.2157, 0.1725,  ..., 0.0941, 0.1294, 0.0824]],\n",
      "\n",
      "        [[0.4353, 0.4314, 0.4510,  ..., 0.3686, 0.3569, 0.3294],\n",
      "         [0.4392, 0.4275, 0.4431,  ..., 0.3686, 0.3529, 0.3412],\n",
      "         [0.4314, 0.4275, 0.4314,  ..., 0.3843, 0.3686, 0.3451],\n",
      "         ...,\n",
      "         [0.4824, 0.3922, 0.3412,  ..., 0.3765, 0.2510, 0.3333],\n",
      "         [0.4510, 0.3961, 0.3333,  ..., 0.3216, 0.3176, 0.2510],\n",
      "         [0.4157, 0.4078, 0.3490,  ..., 0.3020, 0.3255, 0.2627]],\n",
      "\n",
      "        [[0.1922, 0.1804, 0.1961,  ..., 0.1373, 0.1412, 0.1255],\n",
      "         [0.2000, 0.1529, 0.1765,  ..., 0.1216, 0.1255, 0.1333],\n",
      "         [0.1804, 0.1294, 0.1412,  ..., 0.1333, 0.1333, 0.1255],\n",
      "         ...,\n",
      "         [0.6941, 0.5804, 0.5333,  ..., 0.5686, 0.4196, 0.4980],\n",
      "         [0.6588, 0.5765, 0.5176,  ..., 0.5098, 0.4902, 0.4196],\n",
      "         [0.6275, 0.5843, 0.5176,  ..., 0.4824, 0.5020, 0.4275]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FAA367C00A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "with open ('./adversarial/CW_0.2_adv_images.pickle', 'rb') as fp:\n",
    "    info = pickle.load(fp)\n",
    "    print(info[0])\n",
    "    img1 = transforms.ToPILImage()(info[0].float())\n",
    "    img2 = transforms.ToTensor()(img1)\n",
    "    print(img2)\n",
    "    img1.show()\n",
    "with open ('./adversarial/CW_0.2_adv_label.pickle', 'rb') as fp:\n",
    "    info = pickle.load(fp)\n",
    "    print(info[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
